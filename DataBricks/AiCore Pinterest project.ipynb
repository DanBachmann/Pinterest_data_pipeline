{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b135d03-3ca4-41ab-b2d9-22c0b1f86744",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Each markdown block indicates a new section which is a prerequisite for the next.\n",
    "\n",
    "First we check the file system for the access keys, then load the keys and mount the S3 bucket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8869ac6c-c432-4720-a8fd-8be6a9d047d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterId\")) # we'll need this to configure the DAG in AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffc5f34-9946-4550-b300-5cfe557ec5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# check we can access where the access keys are stored\n",
    "dbutils.fs.ls(delta_table_path)\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "# AWS S3 bucket name\n",
    "AWS_S3_BUCKET = \"user-0a1153066525-bucket\"\n",
    "# Mount name for the bucket\n",
    "MOUNT_NAME = \"/mnt/incoming\"\n",
    "# MOUNT_NAME = \"/mnt/dan_bucket\"\n",
    "# Source url\n",
    "SOURCE_URL = \"s3n://{0}:{1}@{2}\".format(ACCESS_KEY, ENCODED_SECRET_KEY, AWS_S3_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced082c1-7f26-4994-b77c-a0942f6de980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mount the drive\n",
    "dbutils.fs.mount(SOURCE_URL, MOUNT_NAME)\n",
    "\n",
    "# test it mounted\n",
    "# display(dbutils.fs.ls(MOUNT_NAME))\n",
    "display(dbutils.fs.ls(MOUNT_NAME + \"/../..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c16377-4409-40e2-bc85-82c4ccddb334",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we load the files from the 3 streams each into a Spark data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba67201a-4dc0-47ba-b0d4-79539eda606a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_type = \"json\"\n",
    "# Ask Spark to infer the schema\n",
    "infer_schema = \"true\"\n",
    "\n",
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = MOUNT_NAME + \"/topics/0a1153066525.pin/partition=0/*.json\" \n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_pin = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "# display works best for many columns as it presents a scrollbar and also the type of each column is indicated\n",
    "display(df_pin.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92163d77-19c6-47fe-91ce-8d8f58897505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = MOUNT_NAME + \"/topics/0a1153066525.geo/partition=0/*.json\" \n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_geo = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "df_geo.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a942382-8610-41e0-9bfa-2a20b933428b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Asterisk(*) indicates reading all the content of the specified file that have .json extension\n",
    "file_location = MOUNT_NAME + \"/topics/0a1153066525.user/partition=0/*.json\" \n",
    "# Read in JSONs from mounted S3 bucket\n",
    "df_user = spark.read.format(file_type) \\\n",
    ".option(\"inferSchema\", infer_schema) \\\n",
    ".load(file_location)\n",
    "# Display Spark dataframe to check its content\n",
    "df_user.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5098446e-795f-4978-a71a-0cc314d75fca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Clean the data in each data frame as per specification. Note that these cells transform the data so it may be required to rerun the dataframe loading if you wish to rerun a cleanup transformation cell. \n",
    "Cleaning functions have been moved to a Github repository for version control and sharing with other data connections such as in the Kinesis assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e352c751-be6f-4476-b07d-89f02b284421",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import PinterestTransformations\n",
    "\n",
    "# clean the df_pin DataFrame\n",
    "df_pin = PinterestTransformations.clean_pin(df_pin)\n",
    "display(df_pin.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27da5f39-a287-4150-b0b1-76c8115195d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean the df_geo DataFrame\n",
    "df_geo = PinterestTransformations.clean_geo(df_geo)\n",
    "df_geo.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35ed2106-8046-4e79-86b6-c9bdde9cbbcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean the df_user DataFrame\n",
    "df_user = PinterestTransformations.clean_geo(df_user)\n",
    "df_user.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36f9d725-9b7d-4030-ad1f-b5bae36609ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Below are report requests. The begining of each code block will have the reporting requirements stated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6de091c1-60d8-43ee-9921-b3ebccd4c8bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the most popular Pinterest category people post to based on their country.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     country\n",
    "#     category\n",
    "#     category_count, a new column containing the desired query output\n",
    "\n",
    "# join pin and geo tables so we have country and category together\n",
    "combined_geo_df = df_pin.join(df_geo, df_geo[\"ind\"] == df_pin[\"ind\"], how=\"inner\")\n",
    "# display(combined_geo_df)\n",
    "# count category use per country\n",
    "grouped_df = combined_geo_df.groupBy(\"country\",\"category\").agg(count(\"category\")).withColumnRenamed(\"count(category)\", \"category_count\")\n",
    "# display(grouped_df)\n",
    "# get the maximum value for each country\n",
    "max_df = grouped_df.groupBy(\"country\").agg(max(\"category_count\")).withColumnRenamed(\"country\", \"max_country\")\n",
    "# display(max_df)\n",
    "# filter to only the maximum category per country\n",
    "most_popular_category_per_country_df = max_df.join(grouped_df, ((grouped_df[\"country\"] == max_df[\"max_country\"]) & (grouped_df[\"category_count\"] == max_df[\"max(category_count)\"])), how=\"inner\")\n",
    "# display(most_popular_category_per_country)\n",
    "# Selecting only the relevant columns\n",
    "most_popular_category_per_country_df = most_popular_category_per_country_df.select(\"country\", \"category\", \"category_count\")\n",
    "display(most_popular_category_per_country_df.orderBy(\"country\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b75c9fb1-0ba1-42b6-b394-9dbbd73844e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find how many posts each category had between 2018 and 2022.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     post_year, a new column that contains only the year from the timestamp column\n",
    "#     category\n",
    "#     category_count, a new column containing the desired query output\n",
    "\n",
    "# combined_geo_df already has timestamp and category together so we'll use it again here\n",
    "# create post_year column which will be of type in\n",
    "grouped_df = combined_geo_df.withColumn('post_year', year(combined_geo_df[\"timestamp\"]))\n",
    "# filter out requested years and count categories per country/year after filter\n",
    "grouped_df = grouped_df.where(\"post_year >= 2018 and post_year <= 2022\").groupBy(\"post_year\",\"category\").agg(count(\"category\")).withColumnRenamed(\"count(category)\", \"category_count\")\n",
    "display(grouped_df.orderBy(\"post_year\", \"category\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6586eba9-4789-43df-a8ee-7ddd1922720e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: For each country find the user with the most followers.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     country\n",
    "#     poster_name\n",
    "#     follower_count\n",
    "\n",
    "# combined_geo_df already has country and poster_name together so we'll use it again here\n",
    "df_pin_sum_followers = combined_geo_df.groupBy(\"country\", \"poster_name\").agg(sum(\"follower_count\")).withColumnRenamed(\"sum(follower_count)\", \"follower_count\")\n",
    "#display(df_pin_sum_followers)\n",
    "df_pin_max_followers = df_pin_sum_followers.groupBy(\"country\").agg(max(\"follower_count\")).withColumnRenamed(\"country\", \"max_country\")\n",
    "#display(df_pin_max_followers)\n",
    "# filter to only the most follower user per country\n",
    "most_followed_per_country_df = df_pin_max_followers.join(df_pin_sum_followers, ((df_pin_sum_followers[\"country\"] == df_pin_max_followers[\"max_country\"]) & (df_pin_sum_followers[\"follower_count\"] == df_pin_max_followers[\"max(follower_count)\"])), how=\"inner\")\n",
    "#display(most_followed_per_country_df)\n",
    "# filter to relevant columns\n",
    "most_followed_per_country_df = most_followed_per_country_df.select(\"country\", \"poster_name\", \"follower_count\")\n",
    "# display sorted by country - confirm one poster_name per country\n",
    "display(most_followed_per_country_df.orderBy(\"country\"))\n",
    "\n",
    "# Step 2: Based on the above query, find the country with the user with most followers.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     country\n",
    "#     follower_count\n",
    "# This DataFrame should have only one entry.\n",
    "country_with_user_with_most_followers_df = most_followed_per_country_df.select(\"country\", \"follower_count\").orderBy(\"follower_count\").tail(1)\n",
    "display(country_with_user_with_most_followers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6449f2-f120-4eaa-9f3b-93f345796661",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# find the most popular category people post to based on the following age groups:\n",
    "#     18-24\n",
    "#     25-35\n",
    "#     36-50\n",
    "#     +50\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     category\n",
    "#     category_count, a new column containing the desired query output\n",
    "\n",
    "# join user with age field to piin with category field\n",
    "combined_user_df = df_pin.join(df_user, df_user[\"ind\"] == df_pin[\"ind\"], how=\"inner\")\n",
    "# create age_group - note this will be used by other report requests in cells below\n",
    "combined_user_df = combined_user_df.withColumn(\"age_group\",when(combined_user_df.age > 50, '+50').when(combined_user_df.age > 35, '36-50').when(combined_user_df.age > 24, '25-35').when(combined_user_df.age > 17, '18-24').otherwise('other'))\n",
    "\n",
    "# back to specifics to this reporting request\n",
    "user_category_count_df = combined_user_df.groupBy(\"age_group\",\"category\").agg(count(\"category\")).withColumnRenamed(\"count(category)\", \"category_count\")\n",
    "# display(user_category_count_df)\n",
    "\n",
    "# get the maximum category count per age group\n",
    "user_category_max_df = user_category_count_df.groupBy(\"age_group\").agg(max(\"category_count\")).withColumnRenamed(\"age_group\",\"max_age_group\")\n",
    "# display(user_category_max_df)\n",
    "# filter to keep only the maximum category count per age group\n",
    "user_category_max_df = user_category_count_df.join(user_category_max_df, (user_category_max_df['max_age_group'] == user_category_count_df['age_group']) & (user_category_max_df['max(category_count)'] == user_category_count_df['category_count']), how=\"inner\")\n",
    "# display(user_category_max_df)\n",
    "# reduce to the columns as per specification\n",
    "user_category_max_df = user_category_max_df.select(\"age_group\", \"category\", \"category_count\")\n",
    "display(user_category_max_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5769af33-d284-45a5-abed-bc9f92426ca4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the median follower count for users in the following age groups:\n",
    "#     18-24\n",
    "#     25-35\n",
    "#     36-50\n",
    "#     +50\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     median_follower_count, a new column containing the desired query output\n",
    "\n",
    "# we have our age group in our combined_user_df, so we'll use that here\n",
    "# Databricks is using an older version of Spark (3.2.1). With version 3.4.0 and later, the following should work and avoid the \"NameError: name 'median' is not defined\" error\n",
    "# user_median_follower_count = combined_user_df.groupBy(\"age_group\",\"follower_count\").agg(median(\"follower_count\")).withColumnRenamed(\"median(follower_count)\", \"median_follower_count\")\n",
    "# Originally tried counting the number of users per age_group and then selecting the middle index; however, using the window function appears to run faster\n",
    "from pyspark.sql import Window\n",
    "median_window = Window.partitionBy(\"age_group\")\n",
    "first_window = median_window.orderBy(\"follower_count\")                                  # first, order by column we want to compute the median for\n",
    "user_median_follower_count = combined_user_df.withColumn(\"percent_rank\", percent_rank().over(first_window))  # add percent_rank column, percent_rank = 0.5 coressponds to median\n",
    "second_window = median_window.orderBy(pow(user_median_follower_count.percent_rank-0.5, 2))                 # order by (percent_rank - 0.5)^2 ascending\n",
    "user_median_follower_count = user_median_follower_count.withColumn(\"median_follower_count\", first(\"follower_count\").over(second_window))     # the first row of the window corresponds to median\n",
    "# display(user_median_follower_count)\n",
    "user_median_follower_count = user_median_follower_count.select(\"age_group\",\"median_follower_count\").distinct()\n",
    "display(user_median_follower_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "531f0426-b7b8-4274-8d58-5f2fdc9a84f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find how many users have joined between 2015 and 2020.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     post_year, a new column that contains only the year from the timestamp column\n",
    "#     number_users_joined, a new column containing the desired query output\n",
    "# note that users could appear in multiple years since they may have joined and posted in one year and continued to post in other years\n",
    "\n",
    "# reduce to only the rows we want first\n",
    "df_user_2015to2020 = df_user.withColumn('joined_year', year(df_user[\"date_joined\"])).where(\"joined_year >= 2015 and joined_year <= 2020\").withColumnRenamed(\"ind\", \"ind_user\")\n",
    "# join to the other tables - note this will be used by other reporting requests in the cells below\n",
    "users_joined_by_year_2015to2020_df = df_geo.join(df_user_2015to2020, df_user_2015to2020[\"ind_user\"] == df_geo[\"ind\"], how=\"inner\") # for timestamp\n",
    "\n",
    "# back to specifics to this reporting request\n",
    "# create the post_year column\n",
    "users_joined_by_year_2015to2020_df = users_joined_by_year_2015to2020_df.withColumn('post_year', year(users_joined_by_year_2015to2020_df[\"timestamp\"]))\n",
    "# reduce to the columns we want which make a unique user and keep the distinct list\n",
    "users_joined_by_year_2015to2020_df_summary = users_joined_by_year_2015to2020_df.select(\"post_year\",\"age\",\"date_joined\",\"user_name\").distinct()\n",
    "# display(users_joined_by_year_2015to2020_df_summary)\n",
    "users_joined_by_year_2015to2020_df_summary = users_joined_by_year_2015to2020_df_summary.groupBy(\"post_year\").agg(count(\"date_joined\")).withColumnRenamed(\"count(date_joined)\", \"number_users_joined\")\n",
    "display(users_joined_by_year_2015to2020_df_summary.orderBy(\"post_year\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c1c972-f7d8-4bec-8436-2fb5be1ab5fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the median follower count of users have joined between 2015 and 2020.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     post_year, a new column that contains only the year from the timestamp column\n",
    "#     median_follower_count, a new column containing the desired query output\n",
    "\n",
    "# Databricks is using an older version of Spark (3.2.1). With version 3.4.0 and later, the median function can be used. The current version of Spark is 3.5.\n",
    "\n",
    "# we already have the data filtered, tables joined and post_year in users_joined_by_year_2015to2020_df so we'll re-use that\n",
    "records_combined_year_2015to2020_df = df_pin.join(users_joined_by_year_2015to2020_df, users_joined_by_year_2015to2020_df[\"ind_user\"] == df_pin[\"ind\"], how=\"inner\") # for follower_count\n",
    "\n",
    "median_window = Window.partitionBy(\"post_year\")\n",
    "first_window = median_window.orderBy(\"follower_count\")                                  # first, order by column we want to compute the median for\n",
    "median_user_follow_count_by_year_2015to2020_df = records_combined_year_2015to2020_df.withColumn(\"percent_rank\", percent_rank().over(first_window))  # add percent_rank column, percent_rank = 0.5 coressponds to median\n",
    "second_window = median_window.orderBy(pow(median_user_follow_count_by_year_2015to2020_df.percent_rank-0.5, 2))                 # order by (percent_rank - 0.5)^2 ascending\n",
    "median_user_follow_count_by_year_2015to2020_df = median_user_follow_count_by_year_2015to2020_df.withColumn(\"median_follower_count\", first(\"follower_count\").over(second_window))     # the first row of the window corresponds to median\n",
    "# display(median_user_follow_count_by_year_2015to2020_df)\n",
    "median_user_follow_count_by_year_2015to2020_df = median_user_follow_count_by_year_2015to2020_df.select(\"post_year\",\"median_follower_count\").distinct()\n",
    "display(median_user_follow_count_by_year_2015to2020_df.orderBy(\"post_year\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211eb901-3025-416a-b0a1-dbc329e25e00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Find the median follower count of users that have joined between 2015 and 2020, based on which age group they are part of.\n",
    "# return a DataFrame that contains the following columns:\n",
    "#     age_group, a new column based on the original age column\n",
    "#     post_year, a new column that contains only the year from the timestamp column\n",
    "#     median_follower_count, a new column containing the desired query output\n",
    "\n",
    "# Databricks is using an older version of Spark (3.2.1). With version 3.4.0 and later, the median function can be used. The current version of Spark is 3.5.\n",
    "\n",
    "# we already have the data filtered, tables joined and post_year in records_combined_year_2015to2020_df so we'll re-use that and just add the age_group to it\n",
    "records_combined_age_group_2015to2020_df = records_combined_year_2015to2020_df.withColumn(\"age_group\",when(records_combined_year_2015to2020_df.age > 50, '+50').when(records_combined_year_2015to2020_df.age > 35, '36-50').when(records_combined_year_2015to2020_df.age > 24, '25-35').when(records_combined_year_2015to2020_df.age > 17, '18-24').otherwise('other'))\n",
    "\n",
    "median_window = Window.partitionBy(\"post_year\",\"age_group\")\n",
    "first_window = median_window.orderBy(\"follower_count\")                                  # first, order by column we want to compute the median for\n",
    "median_user_follow_count_by_year_2015to2020_df = records_combined_age_group_2015to2020_df.withColumn(\"percent_rank\", percent_rank().over(first_window))  # add percent_rank column, percent_rank = 0.5 coressponds to median\n",
    "second_window = median_window.orderBy(pow(median_user_follow_count_by_year_2015to2020_df.percent_rank-0.5, 2))                 # order by (percent_rank - 0.5)^2 ascending\n",
    "median_user_follow_count_by_year_2015to2020_df = median_user_follow_count_by_year_2015to2020_df.withColumn(\"median_follower_count\", first(\"follower_count\").over(second_window))     # the first row of the window corresponds to median\n",
    "# display(median_user_follow_count_by_year_2015to2020_df)\n",
    "median_user_follow_count_by_year_2015to2020_df = median_user_follow_count_by_year_2015to2020_df.select(\"post_year\", \"age_group\",\"median_follower_count\").distinct()\n",
    "display(median_user_follow_count_by_year_2015to2020_df.orderBy(\"post_year\", \"age_group\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5e1879-1c38-461b-813e-28118cb67781",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Unmount the drive if we are finished with it. While working with the notebook and running selective cells, the filesystem should remain mounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e060153b-d3a1-4650-8e84-cdccae4da805",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(MOUNT_NAME)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AiCore Pinterest project",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

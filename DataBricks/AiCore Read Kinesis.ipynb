{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ffc5f34-9946-4550-b300-5cfe557ec5ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import urllib\n",
    "\n",
    "# Define the path to the Delta table\n",
    "delta_table_path = \"dbfs:/user/hive/warehouse/authentication_credentials\"\n",
    "\n",
    "# Read the Delta table to a Spark DataFrame\n",
    "aws_keys_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.select('Access key ID').collect()[0]['Access key ID']\n",
    "SECRET_KEY = aws_keys_df.select('Secret access key').collect()[0]['Secret access key']\n",
    "# Encode the secrete key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")\n",
    "\n",
    "aws_user = '0a1153066525'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61909579-eb45-4eaa-ac13-990295a09546",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "suffix='pin'\n",
    "stream_name = 'streaming-'+ aws_user + '-' + suffix\n",
    "df_pin = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName',stream_name) \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "suffix='geo'\n",
    "stream_name = 'streaming-'+ aws_user + '-' + suffix\n",
    "df_geo = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName',stream_name) \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n",
    "\n",
    "suffix='user'\n",
    "stream_name = 'streaming-'+ aws_user + '-' + suffix\n",
    "df_user = spark \\\n",
    ".readStream \\\n",
    ".format('kinesis') \\\n",
    ".option('streamName',stream_name) \\\n",
    ".option('initialPosition','earliest') \\\n",
    ".option('region','us-east-1') \\\n",
    ".option('awsAccessKey', ACCESS_KEY) \\\n",
    ".option('awsSecretKey', SECRET_KEY) \\\n",
    ".load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9026f268-2047-4c0c-a1b6-ff36b1b241b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import PinterestTransformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ced082c1-7f26-4994-b77c-a0942f6de980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean the df_pin DataFrame\n",
    "# Define the schema for parsing the JSON\n",
    "# original raw data structure before transformation\n",
    "raw_pin_columns = {\n",
    "    \"category\",\n",
    "    \"description\", \n",
    "    \"downloaded\", \n",
    "    \"follower_count\",\n",
    "    \"image_src\", \n",
    "    \"index\", \n",
    "    \"is_image_or_video\", \n",
    "    \"poster_name\", \n",
    "    \"save_location\", \n",
    "    \"tag_list\",\n",
    "    \"title\", \n",
    "    \"unique_id\"\n",
    "}\n",
    "# could be improved in refactoring by having a list of string types and long types, then build the schema and extract the columns so column names do not need to be repeated in code\n",
    "raw_pin_json_schema = StructType([\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"downloaded\", LongType(), True),\n",
    "    StructField(\"follower_count\", StringType(), True),\n",
    "    StructField(\"image_src\", StringType(), True),\n",
    "    StructField(\"index\", LongType(), True),\n",
    "    StructField(\"is_image_or_video\", StringType(), True),\n",
    "    StructField(\"poster_name\", StringType(), True),\n",
    "    StructField(\"save_location\", StringType(), True),\n",
    "    StructField(\"tag_list\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"unique_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract specific fields from the parsed JSON\n",
    "# could be improved in refactoring by moving the rest of this to a function since it is the same for every stream\n",
    "df_pin = df_pin.selectExpr(\"CAST(data as STRING)\")\n",
    "df_pin = df_pin.withColumn(\"parsed_json\", from_json(df_pin.data, raw_pin_json_schema))\n",
    "for column in raw_pin_columns:\n",
    "    df_pin = df_pin.withColumn(column, df_pin.parsed_json[column])\n",
    "df_pin = df_pin.drop(\"data\",\"parsed_json\")\n",
    "\n",
    "df_pin = PinterestTransformations.clean_pin(df_pin)\n",
    "# display(df_pin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "850451b9-ce5a-4b67-a6e3-6a7b7f61902d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean the df_geo DataFrame\n",
    "# Define the schema for parsing the JSON\n",
    "# original raw data structure before transformation\n",
    "raw_geo_columns = {\n",
    "    \"country\",\n",
    "    \"ind\", \n",
    "    \"latitude\", \n",
    "    \"longitude\",\n",
    "    \"timestamp\"\n",
    "}\n",
    "raw_geo_json_schema = StructType([\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"ind\", LongType(), True),\n",
    "    StructField(\"latitude\", DoubleType(), True),\n",
    "    StructField(\"longitude\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract specific fields from the parsed JSON\n",
    "df_geo = df_geo.selectExpr(\"CAST(data as STRING)\")\n",
    "df_geo = df_geo.withColumn(\"parsed_json\", from_json(df_geo.data, raw_geo_json_schema))\n",
    "for column in raw_geo_columns:\n",
    "    df_geo = df_geo.withColumn(column, df_geo.parsed_json[column])\n",
    "df_geo = df_geo.drop(\"data\",\"parsed_json\")\n",
    "\n",
    "df_geo = PinterestTransformations.clean_geo(df_geo)\n",
    "# display(df_geo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fa04204-c886-4c3c-b0b5-700dcb7f8a62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean the df_user DataFrame\n",
    "# Define the schema for parsing the JSON\n",
    "# original raw data structure before transformation\n",
    "raw_user_columns = {\n",
    "    \"age\",\n",
    "    \"date_joined\", \n",
    "    \"first_name\", \n",
    "    \"ind\",\n",
    "    \"last_name\"\n",
    "}\n",
    "raw_user_json_schema = StructType([\n",
    "    StructField(\"age\", LongType(), True),\n",
    "    StructField(\"date_joined\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"ind\", LongType(), True),\n",
    "    StructField(\"last_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Extract specific fields from the parsed JSON\n",
    "df_user = df_user.selectExpr(\"CAST(data as STRING)\")\n",
    "df_user = df_user.withColumn(\"parsed_json\", from_json(df_user.data, raw_user_json_schema))\n",
    "for column in raw_user_columns:\n",
    "    df_user = df_user.withColumn(column, df_user.parsed_json[column])\n",
    "df_user = df_user.drop(\"data\",\"parsed_json\")\n",
    "\n",
    "df_user = PinterestTransformations.clean_user(df_user)\n",
    "# display(df_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d62b1d4d-3df6-415d-b50d-4aa131eaa2ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# write data into DataBricks.\n",
    "\n",
    "df_pin.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(aws_user + \"_pin_table\")\n",
    "\n",
    "df_geo.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(aws_user + \"_geo_table\")\n",
    "\n",
    "df_user.writeStream \\\n",
    "  .format(\"delta\") \\\n",
    "  .outputMode(\"append\") \\\n",
    "  .option(\"checkpointLocation\", \"/tmp/kinesis/_checkpoints/\") \\\n",
    "  .table(aws_user + \"_user_table\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AiCore Read Kinesis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
